{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD7 Particle filtering\n",
    "This notebook is done for personal training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 - Extention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following part, we will do the URL prediction by considering process noise, which is supposed to be following a normal distribution. This will be processed by 2 ways, a less cost but inaccurate one and a more costly one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random distribute approach\n",
    "For this approach, we will and a process noise randomly, this approach isn't so accurate but will be is costless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define several useful functions:   \n",
    "- ```normalisation``` is used to normalise all the particles.\n",
    "- ```iteration``` is the iteration formula.\n",
    "- ```average``` is to compute the average according to the probability.\n",
    "- ```multiplication``` is to multiply the particle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalisation(particles):\n",
    "    sums = 0\n",
    "    for i in range(len(particles)):\n",
    "        sums += particles[i][1]\n",
    "    for i in range(len(particles)):\n",
    "        particles[i][1]=particles[i][1]/sums\n",
    "\n",
    "def iteration_simple(particle):\n",
    "    particle[0] = 2*particle[0]+np.random.normal()\n",
    "    return particle\n",
    "\n",
    "def average(arrays):\n",
    "    value = 0\n",
    "    for i in arrays:\n",
    "        value += i[0]*i[1]\n",
    "    return value\n",
    "\n",
    "def multiplication(particles, n=1):\n",
    "    k=len(particles)\n",
    "    for i in range(k):\n",
    "        for _ in range(1,n):\n",
    "            particles.append(particles[i].copy())\n",
    "    normalisation(particles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.8"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "particles=[[1.5,0.2],[2,0.3],[2.1,0.3],[3,0.1],[3.5,0.1],[1.8,0.1]]\n",
    "\n",
    "def compute_URL_simple(particles,seuil):\n",
    "    URLs=[]\n",
    "    for i in particles:\n",
    "        k=0\n",
    "        while i[0]<=seuil:\n",
    "            i = iteration_simple(i)\n",
    "            k += 1\n",
    "        URLs.append([k,i[1]])\n",
    "    return URLs\n",
    "\n",
    "URLs = compute_URL_simple(particles, 100)\n",
    "average(URLs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can become much more precise, by multiplying the particle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.499999999999999"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "particles=[[1.5,0.2],[2,0.3],[2.1,0.3],[3,0.1],[3.5,0.1],[1.8,0.1]]\n",
    "multiplication(particles,n=2)\n",
    "URLs = compute_URL_simple(particles, 100)\n",
    "average(URLs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is note a convergent method, as is shown in the following graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLmax=[]\n",
    "URLmin=[]\n",
    "\n",
    "for i in range(1,10):\n",
    "    results = []\n",
    "    particles=[[1.5,0.2],[2,0.3],[2.1,0.3],[3,0.1],[3.5,0.1],[1.8,0.1]]\n",
    "    multiplication(particles,n=i)\n",
    "    for _ in range(20):\n",
    "        pq = particles.copy()\n",
    "        URLs = compute_URL_simple(pq, 100)\n",
    "        results.append(average(URLs))\n",
    "    URLmax.append(max(results))\n",
    "    URLmin.append(min(results))\n",
    "    print(URLs)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "# Signal\n",
    "ax.plot(range(1,10), URLmax, 'b', label='URLmax')\n",
    "ax.plot(range(1,10), URLmin, 'k', label='URLmin')\n",
    "ax.set_xlabel('URL')\n",
    "ax.set_ylabel('Number of multiplication')\n",
    "ax.grid(True)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF distribute approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme of Particle Filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.163636363636375"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
